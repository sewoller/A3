[This](obsidian://open?vault=A3&file=Attention%20in%20Psychology%2C%20Neuroscience%2C%20and%20Machine%20Learning%2Ffncom-14-00029.pdf) paper discusses attention in the context of Neuroscience and Psychology and draw parallels to applications of [[attention]] in Machine Learning. It defines attention as the flexible control of limited computational resources. In the context of humans, computational resources would refer to our ability to carry out cognition. In the context of machine learning, computational resources relate more literally to the system resources available to perform computations.

The interesting part of this article is how connections are drawn between models of human attention and how they can be applied to improve Machine Learning models. In the same way that the inputs and outputs of a neural network are modelled after the neurons in a human brain, models of attention can be applied to these neural networks to make these systems more dynamic and adaptable.

This article details the different kinds of sensory attention: 
1) Visual-Spatial, 
2) Visual Feature
3) Other sensory modalities of attention:
	1) Hearing - [The Cocktail Party Effect / Problem](https://en.wikipedia.org/wiki/Cocktail_party_effect)
	2) Somatosensory - Attention to touch
	3) Taste
	4) Combinations of sensory attention

It also discusses at length the relationship between attention, decision making (executive control) and memory. The discussion is again focused primarily on the sharing of resources between these different modes of cognition.

Finally the article goes into detail on how the knowledge of attention mentioned above have been applied to improve machine learning models including visual processing, natural language processing and multi-task computation. It continues to discuss applications of these theories including investigations into strategies we might employ to enhance attention in the future.