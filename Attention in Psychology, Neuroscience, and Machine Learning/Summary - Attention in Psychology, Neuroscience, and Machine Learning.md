This paper discusses attention in the context of Neuroscience and Psychology and draw parallels to applications of attention in Machine Learning. It defines attention as the flexible control of limited computational resources. In the context of humans, computational resources would refer to our ability to carry out cognition. In the context of machine learning, computational resources relate more literally to the system resources available to perform computations.

The interesting part of this article is how connections are drawn between models of human attention and how they can be applied to improve Machine Learning models. In the same way that the inputs and outputs of a neural network are modelled after the neurons in a human brain, models of attention can be applied to these neural networks to make these systems more dynamic and adaptable.

This article details the different kinds of sensory attention: 
1) Visual-Spatial, 
2) Visual Feature
3) Other sensory modalities of attention including hearing